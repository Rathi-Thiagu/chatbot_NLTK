{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"chatbot_NLTK.ipynb","provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyNj6jIAfY4jOWew8JNwKpEc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"HpqhiSPux5kX"},"source":["    <!-- Chatbots : \n","    sample_1:\n","    Top applications of today : \n","      Helpdesk assistant \n","      Email distributor \n","      Home assistant \n","      Operations Assistant \n","      Phone assistant ..siri , google , alexa \n","      Entertainment assistant .. football , basketball, predictions on matches etc\n","\n","    <!-- Architecture of Chatbots : \n","      <!-- chat window or session -- Interface --- NLP model ---corpus or Application DB \n","    How does a chatbot work ?\n","        Import corpus ---needed for training the model  \n","        Preprocess the data --- \n","        Text case handling ---convert all the data coming as input to either upper case or lower \n","        Tokenization --- This is a blog \n","        Stemming --- finding similarities between words with same root word \n","        Bag of Words (BOW) -- process of converting words into numbers by generating vector embeddings from the tokens generated \n","        One hot encoding *italicized text*"]},{"cell_type":"markdown","metadata":{"id":"-3BtR6d7j53N"},"source":["# Importing the required libraries "]},{"cell_type":"code","metadata":{"id":"tfqwlRq9j8Jt"},"source":["import numpy as np \n","import nltk \n","import string \n","import random"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eCkXHqnxkQ4v"},"source":["# Importing and reading the corpus"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vZRcrugBkQaC","executionInfo":{"status":"ok","timestamp":1632874905787,"user_tz":420,"elapsed":587,"user":{"displayName":"Rathi Thiagu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVgb-dMIcxBmxSGH9ns-gdUdDmfXmD8J-HqDjF71Q=s64","userId":"14096260315487702777"}},"outputId":"a370d9f4-5bf2-4d53-ed8b-686b0b488707"},"source":["f = open('chatbot.txt','r',errors='ignore')\n","raw_doc = f.read()\n","raw_doc = raw_doc.lower() #converts the text lowercase\n","nltk.download('punkt') #using the Punkt dictionary \n","nltk.download('wordnet')#using the WordNet dictionary \n","sent_tokens = nltk.sent_tokenize(raw_doc) #converts doc to list of sentences\n","word_tokens = nltk.word_tokenize(raw_doc) #converts doc to list of words "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"]}]},{"cell_type":"markdown","metadata":{"id":"HL-SzxMMlrOh"},"source":["# Example of sentence tokens"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1yxXWv7FloXj","executionInfo":{"status":"ok","timestamp":1632874905787,"user_tz":420,"elapsed":52,"user":{"displayName":"Rathi Thiagu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVgb-dMIcxBmxSGH9ns-gdUdDmfXmD8J-HqDjF71Q=s64","userId":"14096260315487702777"}},"outputId":"a722b00f-95d7-422c-c516-448aa3492a5d"},"source":["sent_tokens[:2]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['data science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from noisy, structured and unstructured data,[1][2] and apply knowledge and actionable insights from data across a broad range of application domains.',\n"," 'data science is related to data mining, machine learning and big data.']"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"NGOBb4oIl8go"},"source":["# Example of word tokens"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2iGoSA71lygv","executionInfo":{"status":"ok","timestamp":1632874905788,"user_tz":420,"elapsed":43,"user":{"displayName":"Rathi Thiagu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVgb-dMIcxBmxSGH9ns-gdUdDmfXmD8J-HqDjF71Q=s64","userId":"14096260315487702777"}},"outputId":"66c91370-4352-4372-9834-d3840292fa8c"},"source":["word_tokens[:2]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['data', 'science']"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"T8WFIgxYmAiy"},"source":["# Text Preprocessing"]},{"cell_type":"code","metadata":{"id":"IQji3xbvl4vB"},"source":["lemmer = nltk.stem.WordNetLemmatizer()\n","#WordNet is a semantically-oriented dictionary of English included in NLTK\n","def LemTokens(tokens):\n","  return [lemmer.lemmatize(token) for token in tokens]\n","remove_punct_dict = dict((ord(punct),None) for punct in string.punctuation)\n","def LemNormalize(text):\n","  return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XrDkyQhhnRrt"},"source":["# Defining the greeting function"]},{"cell_type":"code","metadata":{"id":"S6n54OPvnBKM"},"source":["GREET_INPUTS=(\"hello\",\"hi\",\"greetings\",\"sup\",\"what's up\",\"hey\")\n","GREET_RESPONSES=[\"hi\",\"hey\",\"*nods*\",\"hi there\",\"hello\",\"I am glas !You are talking to me\"]\n","def greet(sentence):\n","\n","  for word in sentence.split():\n","    if word.lower() in GREET_INPUTS:\n","      return random.choice(GREET_RESPONSES)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l4QrU40Un9FZ"},"source":["# Response Generation"]},{"cell_type":"code","metadata":{"id":"5sisTM49n67q"},"source":["#term frequency - frequency fo occurence of word \n","# idf - inverse document frequency --how arrae is the word in the document \n","# cosine_similarity -- gives the similarity between the words by . product \n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x8RuwjNjoqzz"},"source":["def response(user_response):\n","  robol_response=''\n","  TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n","  tfidf = TfidfVec.fit_transform(sent_tokens)\n","  vals = cosine_similarity(tfidf[-1],tfidf)\n","  idx=vals.argsort()[0][-2]\n","  flat = vals.flatten()\n","  flat.sort()\n","  req_tfidf=flat[-2]\n","  if(req_tfidf==0):\n","    robol_response=robol_response+\"I am sorry! I don't understand you\"\n","    return robol_response\n","  else:\n","    robol_response=robol_response+sent_tokens[idx]\n","    return robol_response"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kjqgYyvDqAaE"},"source":["# Defining conversation start/end protocols\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"19XW17h9p-pi","executionInfo":{"status":"ok","timestamp":1632874979095,"user_tz":420,"elapsed":73328,"user":{"displayName":"Rathi Thiagu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVgb-dMIcxBmxSGH9ns-gdUdDmfXmD8J-HqDjF71Q=s64","userId":"14096260315487702777"}},"outputId":"e2b26b00-990f-4756-b38a-2c24e524452a"},"source":["flag = True \n","print(\"BOT:My name is Stark. Let's have a conversation!Also, if you want to exit any time,just type Bye!\")\n","while(flag==True):\n","  user_response=input()\n","  user_response=user_response.lower()\n","  if(user_response!=\"bye\"):\n","    if(user_response==\"thanks\" or user_response==\"thank you\"):\n","      flag = False\n","      print(\"BOT: you are welcome..\")\n","    else:\n","      if(greet(user_response)!=None):\n","        print(\"BOT: \"+greet(user_response))\n","      else:\n","        print(\"entered here\")\n","        sent_tokens.append(user_response)\n","        word_tokens=word_tokens+nltk.word_tokenize(user_response)\n","        final_words = list(set(word_tokens))\n","        print(\"BOT: \",end=\"\")\n","        print(response(user_response))\n","        sent_tokens.remove(user_response)\n","  else:\n","    flag = False\n","    print(\"BOT: Goodbye! Take care <3\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["BOT:My name is Stark. Let's have a conversation!Also, if you want to exit any time,just type Bye!\n","Hi\n","BOT: hello\n","data science\n","entered here\n","BOT: "]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n"]},{"name":"stdout","output_type":"stream","text":["\"data science\".\n","impacts of datascience\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n"]},{"name":"stdout","output_type":"stream","text":["entered here\n","BOT: \"the impacts of big data that you may not have heard of\".\n","fundations of data science \n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n","  'stop_words.' % sorted(inconsistent))\n"]},{"output_type":"stream","name":"stdout","text":["entered here\n","BOT: \"data science\".\n","bye\n","BOT: Goodbye! Take care <3\n"]}]}]}